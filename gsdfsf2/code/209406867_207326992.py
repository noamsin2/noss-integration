# -*- coding: utf-8 -*-
"""Copy of DL2.ipynb
dfsfds
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nPpEy5ll-BmQgxmA0dziRh6S7Pbxig5t
"""

import numpy as np
import pandas as pd
import string
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk import ngrams
from collections import Counter

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import Adam, RMSprop, Adagrad, Adadelta, Adamax, Nadam, SGD
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, MaxAbsScaler, QuantileTransformer, PowerTransformer, Binarizer
from sklearn.model_selection import train_test_split, KFold
from spellchecker import SpellChecker
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.metrics import accuracy_score
!pip install np_utils
!pip install pyspellchecker
!pip install contractions
!pip install tensorflow~=2.10.0
import np_utils
import contractions
from spellchecker import SpellChecker

def preprocess_text(text):
  lemmatizer = WordNetLemmatizer()
  stop_words = set(stopwords.words('english'))
  text = contractions.fix(text)
  #transform to lower case
  text = text.lower()
  #remove special characters and punctuation
  text = re.sub(r'[^\w\s]', '', text)
  #remove stopwords
  tokens = nltk.word_tokenize(text)
  tokens = [word for word in tokens if word not in stop_words]
  #lemmatization
  tokens = [lemmatizer.lemmatize(word) for word in tokens]
  return ' '.join(tokens)

xlsx = pd.ExcelFile('/content/train_ex2.xlsx')
train_df = pd.read_excel(xlsx,"Sheet1")
xlsx = pd.ExcelFile('/content/test_ex2.xlsx')
test_df = pd.read_excel(xlsx, "Sheet1")
xlsx = pd.ExcelFile('/content/val_ex2.xlsx')
validation_df = pd.read_excel(xlsx, "Sheet1")



for df in (train_df, test_df, validation_df):
    df['text'] = df['text'].apply(preprocess_text)


X_train_texts = train_df['text'].astype(str).values
X_test_texts = test_df['text'].astype(str).values
X_val_texts = validation_df['text'].astype(str).values

vectorizer = CountVectorizer(max_features=10000)
X_train = vectorizer.fit_transform(X_train_texts).toarray()
X_test = vectorizer.transform(X_test_texts).toarray()
X_val = vectorizer.transform(X_val_texts).toarray()
Y_train = train_df['label'].values
Y_val = validation_df['label'].values

add_noise = False


scalers = {
    'StandardScaler': StandardScaler(),
    'MinMaxScaler': MinMaxScaler(),
    'RobustScaler': RobustScaler(),
    'Normalizer': Normalizer(),
    'MaxAbsScaler': MaxAbsScaler(),
    'PowerTransformer': PowerTransformer(method='yeo-johnson'),
}
scaler = scalers['PowerTransformer']
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_val_scaled = scaler.transform(X_val)

"""# autoencoder"""

import keras
from keras import layers
from keras.callbacks import EarlyStopping


input_dim = X_train_scaled.shape[1]
encoding_dim = 128

#previous autoencoder
input_layer = keras.Input(shape=(input_dim, ))
encoded = layers.Dense(512, activation='relu')(input_layer)
encoded = layers.Dense(256, activation='relu')(encoded)
encoded = layers.Dense(encoding_dim, activation='relu')(encoded)
decoded = layers.Dense(256, activation='relu')(encoded)
decoded = layers.Dense(512, activation='relu')(decoded)
decoded = layers.Dense(input_dim, activation='linear')(decoded)

autoencoder = keras.Model(input_layer, decoded)

early_stopping = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True)
autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse',metrics="accuracy")

history = autoencoder.fit(X_train_scaled, X_train_scaled,
                epochs=20,
                batch_size=256,
                shuffle=True,
                validation_data=(X_val_scaled, X_val_scaled),callbacks=[early_stopping])

X_train_encoded = autoencoder.predict(X_train_scaled)
X_val_encoded = autoencoder.predict(X_val_scaled)
X_test_encoded = autoencoder.predict(X_test_scaled)

from keras.regularizers import l2
from keras.layers import Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten
import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold
from keras.models import Sequential, load_model
from keras.layers import Dense
from keras import optimizers

optimizers = [
    Adam(learning_rate=0.001),
    SGD(learning_rate=0.01, momentum=0.9),
    RMSprop(learning_rate=0.001, rho=0.9),
    Adagrad(learning_rate=0.01),
    Adadelta(learning_rate=1.0, rho=0.95),
    Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999),
    Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)
]

activations = ["relu", "sigmoid", "tanh", "softmax", "elu", "swish", "linear"]

optimizers = [Adam(learning_rate=0.001)]
activations = ["elu"]

folder_path = '/content/predictions'
os.makedirs(folder_path, exist_ok=True)
loss = "binary_crossentropy"

file_name_prefix = '/content/predictions/TP_'
all_max_acc = []
input_dim = X_train_encoded.shape[1]
# iterate all optimizers and activations and save the accuracy and loss in a list
for opt in optimizers:
  for activ in activations:

    model = Sequential()
    model.add(Dense(encoding_dim, input_dim=input_dim, activation=activ))
    #model.add(BatchNormalization())
    model.add(Dense(32, activation=activ))
    #model.add(BatchNormalization())
    model.add(Dense(16, activation=activ))
    #model.add(BatchNormalization())
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])
    early_stopping = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True)

    print(f"Training model for optimizer: {opt.__class__.__name__}, activation: {activ}")

    history = model.fit(X_train_encoded, Y_train, epochs=20, batch_size=32, validation_data=(X_val_encoded, Y_val))
    history_max_acc = max(history.history['val_accuracy'])
    history_min_loss = min(history.history['val_loss'])
    all_max_acc.append([history_max_acc, history_min_loss, opt.__class__.__name__, activ])

    train_predictions = model.predict(X_train_encoded).flatten()
    val_predictions = model.predict(X_val_encoded).flatten()
    test_predictions = model.predict(X_test_encoded).flatten()

    # Save the test predictions in a CSV file
    test_predictions_df = pd.DataFrame({'id': test_df['id'], 'predicted_label': test_predictions.flatten()})
    csv_filename = f'{file_name_prefix}_{opt.__class__.__name__}_{activ}.csv'
    test_predictions_df.to_csv(csv_filename, index=False)



    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Model Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Model Loss')

    plt.show()

"""# Save all results to a csv and download it"""

from google.colab import files
columns = ['Max Validation Accuracy', 'Min Validation Loss', 'Optimizer', 'Activation']
all_max_acc_df = pd.DataFrame(all_max_acc, columns=columns)
all_max_acc_df_sorted = all_max_acc_df.sort_values(by='Min Validation Loss', ascending=True)

csv_filename = '/content/all_max_acc_sorted.csv'
all_max_acc_df_sorted.to_csv(csv_filename, index=False)

files.download(csv_filename)

all_max_acc_df_sorted.to_csv('test_predictions_stacked'csv_filename, index=False)
files.download(csv_filename)

"""# Download the test predictions"""

import shutil
from google.colab import files
folder_to_zip = '/content/predictions'
output_filename = '/content/predictions.zip'

shutil.make_archive(output_filename.replace('.zip', ''), 'zip', folder_to_zip)
files.download(output_filename)